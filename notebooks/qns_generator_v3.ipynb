{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import sys\n",
    "from typing import Optional, TypedDict, cast\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "from azure.identity.aio import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.search.documents.aio import SearchClient\n",
    "from azure.search.documents.models import (\n",
    "    QueryCaptionResult,\n",
    "    QueryType,\n",
    "    VectorizedQuery,\n",
    "    VectorQuery,\n",
    ")\n",
    "from dotenv import load_dotenv\n",
    "from openai import AsyncAzureOpenAI\n",
    "from openai_messages_token_helper import build_messages, get_token_limit\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))\n",
    "from app.backend.approaches.prompts import general_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path=r\"..\\.azure\\hhgai-dev-eastasia-002\\.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_SEARCH_SERVICE = os.environ[\"AZURE_SEARCH_SERVICE\"]\n",
    "AZURE_SEARCH_INDEX = os.environ[\"AZURE_SEARCH_INDEX\"]\n",
    "OPENAI_HOST = os.getenv(\"OPENAI_HOST\", \"azure\")\n",
    "OPENAI_CHATGPT_MODEL = os.environ[\"AZURE_OPENAI_CHATGPT_MODEL\"]\n",
    "OPENAI_EMB_MODEL = os.getenv(\"AZURE_OPENAI_EMB_MODEL_NAME\", \"text-embedding-ada-002\")\n",
    "OPENAI_EMB_DIMENSIONS = int(os.getenv(\"AZURE_OPENAI_EMB_DIMENSIONS\", 1536))\n",
    "AZURE_OPENAI_SERVICE = os.getenv(\"AZURE_OPENAI_SERVICE\")\n",
    "AZURE_OPENAI_CHATGPT_DEPLOYMENT = (\n",
    "    os.getenv(\"AZURE_OPENAI_CHATGPT_DEPLOYMENT\") if OPENAI_HOST.startswith(\"azure\") else None\n",
    ")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\") or \"2024-03-01-preview\"\n",
    "AZURE_OPENAI_EMB_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_EMB_DEPLOYMENT\") if OPENAI_HOST.startswith(\"azure\") else None\n",
    "AZURE_OPENAI_CUSTOM_URL = os.getenv(\"AZURE_OPENAI_CUSTOM_URL\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_ORGANIZATION = os.getenv(\"OPENAI_ORGANIZATION\")\n",
    "AZURE_TENANT_ID = os.getenv(\"AZURE_TENANT_ID\")\n",
    "AZURE_USE_AUTHENTICATION = os.getenv(\"AZURE_USE_AUTHENTICATION\", \"\").lower() == \"true\"\n",
    "AZURE_ENFORCE_ACCESS_CONTROL = os.getenv(\"AZURE_ENFORCE_ACCESS_CONTROL\", \"\").lower() == \"true\"\n",
    "AZURE_ENABLE_GLOBAL_DOCUMENT_ACCESS = os.getenv(\"AZURE_ENABLE_GLOBAL_DOCUMENT_ACCESS\", \"\").lower() == \"true\"\n",
    "AZURE_ENABLE_UNAUTHENTICATED_ACCESS = os.getenv(\"AZURE_ENABLE_UNAUTHENTICATED_ACCESS\", \"\").lower() == \"true\"\n",
    "AZURE_SERVER_APP_ID = os.getenv(\"AZURE_SERVER_APP_ID\")\n",
    "AZURE_SERVER_APP_SECRET = os.getenv(\"AZURE_SERVER_APP_SECRET\")\n",
    "AZURE_CLIENT_APP_ID = os.getenv(\"AZURE_CLIENT_APP_ID\")\n",
    "AZURE_AUTH_TENANT_ID = os.getenv(\"AZURE_AUTH_TENANT_ID\", AZURE_TENANT_ID)\n",
    "\n",
    "AZURE_SEARCH_QUERY_LANGUAGE = os.getenv(\"AZURE_SEARCH_QUERY_LANGUAGE\", \"en-us\")\n",
    "AZURE_SEARCH_QUERY_SPELLER = os.getenv(\"AZURE_SEARCH_QUERY_SPELLER\", \"lexicon\")\n",
    "AZURE_SEARCH_SEMANTIC_RANKER = os.getenv(\"AZURE_SEARCH_SEMANTIC_RANKER\", \"free\").lower()\n",
    "\n",
    "CHATGPT_TOKEN_LIMIT = get_token_limit(OPENAI_CHATGPT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_credential = DefaultAzureCredential(exclude_shared_token_cache_credential=True)\n",
    "token_provider = get_bearer_token_provider(azure_credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "openai_client = AsyncAzureOpenAI(\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=f\"https://{AZURE_OPENAI_SERVICE}.openai.azure.com\",\n",
    "    azure_ad_token_provider=token_provider,\n",
    ")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=f\"https://{AZURE_SEARCH_SERVICE}.search.windows.net\",\n",
    "    index_name=AZURE_SEARCH_INDEX,\n",
    "    credential=azure_credential,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "SEARCH_MAX_RESULTS = 30\n",
    "SEARCH_MAX_RESULTS_ARTICLE = 10\n",
    "TEMPERATURE_QNS = 0.3\n",
    "TEMPERATURE_ANS = 0.0\n",
    "SEED = 1234\n",
    "USE_TEXT_SEARCH = \"Hybrid\"\n",
    "USE_VECTOR_SEARCH = \"Hybrid\"\n",
    "USE_SEMANTIC_RANKER = True\n",
    "USE_SEMANTIC_CAPTIONS = False\n",
    "MINIMUM_SEARCH_SCORE = 0.0\n",
    "MINIMUM_RERANKER_SCORE = 0.0\n",
    "RESPONSE_TOKEN_LIMIT = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    id: Optional[str]\n",
    "    parent_id: Optional[str]\n",
    "    title: Optional[str]\n",
    "    pr_name: Optional[str]\n",
    "    cover_image_url: Optional[str]\n",
    "    full_url: Optional[str]\n",
    "    content_category: Optional[str]\n",
    "    chunks: Optional[str]\n",
    "    embedding: Optional[list[float]]\n",
    "    captions: list[QueryCaptionResult]\n",
    "    score: Optional[float] = None\n",
    "    reranker_score: Optional[float] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df():\n",
    "    df = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"content_category\",\n",
    "            \"subpage\",\n",
    "            \"keywords\",\n",
    "            \"source_num\",\n",
    "            \"index_ids\",\n",
    "            \"article_ids_unique\",\n",
    "            \"titles_unique\",\n",
    "            \"content_contributors\",\n",
    "            \"urls_unique\",\n",
    "            \"chunks\",\n",
    "            \"question\",\n",
    "            \"answer\",\n",
    "        ]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_filter_by_content_category(filter_category):\n",
    "    filters = []\n",
    "    filters.append(\"content_category eq '{}'\".format(filter_category.replace(\"'\", \"''\")))\n",
    "    return None if len(filters) == 0 else \" and \".join(filters)\n",
    "\n",
    "\n",
    "def build_filter_by_parent_id(id):\n",
    "    filters = []\n",
    "    id_content = f\"{id}_content\"\n",
    "    id_table = f\"{id}_table\"\n",
    "    id_js = f\"{id}_js\"\n",
    "    filters.append(\"parent_id eq '{}'\".format(id_content.replace(\"'\", \"''\")))\n",
    "    filters.append(\"parent_id eq '{}'\".format(id_table.replace(\"'\", \"''\")))\n",
    "    filters.append(\"parent_id eq '{}'\".format(id_js.replace(\"'\", \"''\")))\n",
    "    return None if len(filters) == 0 else \" or \".join(filters)\n",
    "\n",
    "\n",
    "async def compute_text_embedding(q: str):\n",
    "    SUPPORTED_DIMENSIONS_MODEL = {\n",
    "        \"text-embedding-ada-002\": False,\n",
    "        \"text-embedding-3-small\": True,\n",
    "        \"text-embedding-3-large\": True,\n",
    "    }\n",
    "\n",
    "    class ExtraArgs(TypedDict, total=False):\n",
    "        dimensions: int\n",
    "\n",
    "    dimensions_args: ExtraArgs = (\n",
    "        {\"dimensions\": OPENAI_EMB_DIMENSIONS} if SUPPORTED_DIMENSIONS_MODEL[OPENAI_EMB_MODEL] else {}\n",
    "    )\n",
    "    embedding = await openai_client.embeddings.create(  # noqa: F704\n",
    "        # Azure OpenAI takes the deployment name as the model name\n",
    "        model=AZURE_OPENAI_EMB_DEPLOYMENT if AZURE_OPENAI_EMB_DEPLOYMENT else OPENAI_EMB_MODEL,\n",
    "        input=q,\n",
    "        **dimensions_args,\n",
    "    )\n",
    "    query_vector = embedding.data[0].embedding\n",
    "    return VectorizedQuery(vector=query_vector, k_nearest_neighbors=50, fields=\"embedding\")\n",
    "\n",
    "\n",
    "def get_citation(sourcepage: str, use_image_citation: bool) -> str:\n",
    "    if use_image_citation:\n",
    "        return sourcepage\n",
    "    else:\n",
    "        path, ext = os.path.splitext(sourcepage)\n",
    "        if ext.lower() == \".png\":\n",
    "            page_idx = path.rfind(\"-\")\n",
    "            page_number = int(path[page_idx + 1 :])\n",
    "            return f\"{path[:page_idx]}.pdf#page={page_number}\"\n",
    "\n",
    "        return sourcepage\n",
    "\n",
    "\n",
    "def nonewlines(s: str) -> str:\n",
    "    return s.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "\n",
    "\n",
    "async def get_sources_content(\n",
    "    results: list[Document], use_semantic_captions: bool, use_image_citation: bool\n",
    ") -> list[str]:\n",
    "    if use_semantic_captions:\n",
    "        return [\n",
    "            {\n",
    "                \"id\": doc[\"id\"] or \"\",\n",
    "                \"article_id\": doc[\"parent_id\"] or \"\",\n",
    "                \"title\": doc[\"title\"] or \"\",\n",
    "                \"pr_name\": doc[\"pr_name\"] or \"\",\n",
    "                \"url\": get_citation((doc[\"full_url\"] or \"\"), use_image_citation),\n",
    "                \"chunk\": nonewlines(\" . \".join([cast(str, c.text) for c in (doc[\"captions\"] or [])])),\n",
    "            }\n",
    "            async for doc in results\n",
    "        ]\n",
    "    else:\n",
    "        return [\n",
    "            {\n",
    "                \"index_id\": doc[\"id\"] or \"\",\n",
    "                \"article_id\": doc[\"parent_id\"] or \"\",\n",
    "                \"title\": doc[\"title\"] or \"\",\n",
    "                \"pr_name\": doc[\"pr_name\"] or \"\",\n",
    "                \"url\": get_citation((doc[\"full_url\"] or \"\"), use_image_citation),\n",
    "                \"chunk\": nonewlines(doc[\"chunks\"] or \"\"),\n",
    "            }\n",
    "            async for doc in results\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_sources(sources_content, start_idx, end_idx):\n",
    "    combined = {\"index_ids\": [], \"article_ids\": [], \"titles\": [], \"pr_names\": [], \"urls\": [], \"chunks\": []}\n",
    "\n",
    "    for item in sources_content[start_idx:end_idx]:\n",
    "        combined[\"index_ids\"].append(item[\"index_id\"])\n",
    "        combined[\"article_ids\"].append(item[\"article_id\"])\n",
    "        combined[\"titles\"].append(item[\"title\"])\n",
    "        combined[\"pr_names\"].append(item[\"pr_name\"])\n",
    "        combined[\"urls\"].append(item[\"url\"])\n",
    "        combined[\"chunks\"].append(item[\"chunk\"])\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "def get_combined_sources(sources_content, step=5, total_combined=3):\n",
    "    combined_sources = []\n",
    "    for n in range(0, (total_combined + 1) * step, 5):\n",
    "        combined_source = concat_sources(sources_content, n, min(n + step, len(sources_content)))\n",
    "        combined_sources.append(combined_source)\n",
    "\n",
    "        # Stop when reach total_combined iterations\n",
    "        if len(combined_sources) >= total_combined:\n",
    "            break\n",
    "    return combined_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_df(content_category, percentile):\n",
    "    merged_data = pq.read_table(\"merged_data.parquet\")\n",
    "    merged_data = merged_data.to_pandas()\n",
    "    merged_data_filtered = merged_data[merged_data[\"content_category\"] == content_category]\n",
    "    remove_type_list = [\n",
    "        \"No Extracted Content\",\n",
    "        \"NaN\",\n",
    "        \"No relevant content and mainly links\",\n",
    "        \"Table of Contents\",\n",
    "        \"No HTML Tags\",\n",
    "    ]\n",
    "    merged_data_filtered = merged_data_filtered[~merged_data_filtered[\"remove_type\"].isin(remove_type_list)]\n",
    "    percentile_value = merged_data_filtered[\"page_views\"].quantile(percentile)\n",
    "    df_percentile = merged_data_filtered[merged_data_filtered[\"page_views\"] > percentile_value]\n",
    "    return df_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def retrieve_sources_content(keywords, search_results):\n",
    "    vectors: list[VectorQuery] = []\n",
    "    if USE_VECTOR_SEARCH:\n",
    "        vectors.append(await compute_text_embedding(keywords))\n",
    "\n",
    "    if USE_SEMANTIC_RANKER:\n",
    "        results = await search_client.search(  \n",
    "            search_text=keywords,\n",
    "            filter=filter,\n",
    "            top=search_results,\n",
    "            query_caption=\"extractive|highlight-false\" if USE_SEMANTIC_CAPTIONS else None,\n",
    "            vector_queries=vectors,\n",
    "            query_type=QueryType.SEMANTIC,\n",
    "            query_language=AZURE_SEARCH_QUERY_LANGUAGE,\n",
    "            query_speller=AZURE_SEARCH_QUERY_SPELLER,\n",
    "            semantic_configuration_name=\"default\",\n",
    "            semantic_query=keywords,\n",
    "        )\n",
    "    else:\n",
    "        results = await search_client.search(  \n",
    "            search_text=keywords,\n",
    "            filter=filter,\n",
    "            top=search_results,\n",
    "            vector_queries=vectors,\n",
    "        )\n",
    "\n",
    "    sources_content = await get_sources_content(results, USE_SEMANTIC_CAPTIONS, use_image_citation=False) \n",
    "    return sources_content\n",
    "\n",
    "async def run_chat_completion(input, content, purpose):\n",
    "    if purpose == \"generate_question\":\n",
    "        SYSTEM_PROMPT = qns_generation_prompt.format(keyword=input)\n",
    "        NEW_USER_CONTENT = f\"Please generate 3 unique questions on keywords '{input}' using the following provided source. \\n\\nSource:\\n {content}\"\n",
    "        TEMPERATURE = TEMPERATURE_QNS\n",
    "    elif purpose == \"generate_answer\":\n",
    "        SYSTEM_PROMPT = general_prompt.format(language=\"ENGLISH\")\n",
    "        NEW_USER_CONTENT = input + \"\\n\\nSources:\\n\" + content\n",
    "        TEMPERATURE = TEMPERATURE_ANS     \n",
    "\n",
    "    messages = build_messages(\n",
    "        model=OPENAI_CHATGPT_MODEL,\n",
    "        system_prompt=SYSTEM_PROMPT,\n",
    "        new_user_content=NEW_USER_CONTENT,\n",
    "        max_tokens=CHATGPT_TOKEN_LIMIT - RESPONSE_TOKEN_LIMIT,\n",
    "    )\n",
    "    chat_completion: ChatCompletion = await openai_client.chat.completions.create(\n",
    "        # Azure OpenAI takes the deployment name as the model name\n",
    "        model=AZURE_OPENAI_CHATGPT_DEPLOYMENT,\n",
    "        messages=messages,\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=RESPONSE_TOKEN_LIMIT,\n",
    "        n=1,\n",
    "        stream=False,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    input_tokens_log = chat_completion.usage.prompt_tokens\n",
    "    output_tokens_log = chat_completion.usage.completion_tokens\n",
    "\n",
    "    return chat_completion, input_tokens_log, output_tokens_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_by_content_category(keywords):\n",
    "    input_tokens = 0\n",
    "    output_tokens = 0\n",
    "    df = create_df()\n",
    "    \n",
    "    sources_content = await retrieve_sources_content(keywords, SEARCH_MAX_RESULTS)\n",
    "    combined_sources = get_combined_sources(sources_content, step=5, total_combined=3)\n",
    "\n",
    "    for n in range(len(combined_sources)):\n",
    "        content = \"\\n\".join(combined_sources[n][\"chunks\"])\n",
    "\n",
    "        chat_coroutine, input_tokens_log_q, output_tokens_log_q = await run_chat_completion(keywords, content, \"generate_question\")\n",
    "        response_text_qns = chat_coroutine.choices[0].message.content\n",
    "        questions_list = ast.literal_eval(response_text_qns)\n",
    "\n",
    "        input_tokens += input_tokens_log_q\n",
    "        output_tokens += output_tokens_log_q\n",
    "\n",
    "        data = []        \n",
    "        for question in questions_list:\n",
    "            chat_coroutine, input_tokens_log_a, output_tokens_log_a = await run_chat_completion(question, content, \"generate_answer\")\n",
    "            input_tokens += input_tokens_log_a\n",
    "            output_tokens += output_tokens_log_a\n",
    "\n",
    "            response_text_ans = chat_coroutine.choices[0].message.content\n",
    "            data.append(\n",
    "                {\n",
    "                    \"content_category\": content_category,\n",
    "                    \"subpage\": subpage,\n",
    "                    \"keywords\": keywords,\n",
    "                    \"source_num\": f\"source_{n+1}\",\n",
    "                    \"index_ids\": combined_sources[n][\"index_ids\"],\n",
    "                    \"article_ids_unique\": list(set(combined_sources[n][\"article_ids\"])),\n",
    "                    \"titles_unique\": list(set(combined_sources[n][\"titles\"])),\n",
    "                    \"content_contributors\": list(set(combined_sources[n][\"pr_names\"])),\n",
    "                    \"urls_unique\": list(set(combined_sources[n][\"urls\"])),\n",
    "                    \"chunks\": content,\n",
    "                    \"question\": question,\n",
    "                    \"answer\": response_text_ans,\n",
    "                }\n",
    "            )\n",
    "        df = pd.concat([df, pd.DataFrame(data)], ignore_index=True)\n",
    "    return df, input_tokens, output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_by_page_views(df_percentile, keywords):\n",
    "    input_tokens = 0\n",
    "    output_tokens = 0\n",
    "    df = create_df()\n",
    "    cnt = 1\n",
    "    for index, row in df_percentile.iterrows():\n",
    "        title = row[\"title\"]\n",
    "        id = row[\"id\"]\n",
    "        filter = build_filter_by_parent_id(id)\n",
    "\n",
    "        sources_content = await retrieve_sources_content(keywords, SEARCH_MAX_RESULTS_ARTICLE)\n",
    "        combined = concat_sources(sources_content, 0, len(sources_content))\n",
    "        content = \"\\n\".join(combined[\"chunks\"])\n",
    "\n",
    "        chat_coroutine, input_tokens_log_q, output_tokens_log_q = await run_chat_completion(keywords,content, \"generate_question\")\n",
    "        response_text_qns = chat_coroutine.choices[0].message.content\n",
    "        questions_list = ast.literal_eval(response_text_qns)\n",
    "\n",
    "        input_tokens += input_tokens_log_q\n",
    "        output_tokens += output_tokens_log_q\n",
    "\n",
    "        data = []\n",
    "        for question in questions_list:\n",
    "            chat_coroutine, input_tokens_log_a, output_tokens_log_a = await run_chat_completion(question, content, \"generate_answer\")\n",
    "            response_text_ans = chat_coroutine.choices[0].message.content\n",
    "\n",
    "            input_tokens += input_tokens_log_a\n",
    "            output_tokens += output_tokens_log_a\n",
    "\n",
    "            data.append(\n",
    "                {\n",
    "                    \"content_category\": content_category,\n",
    "                    \"subpage\": subpage,\n",
    "                    \"keywords\": keywords,\n",
    "                    \"source_num\": f\"source_{cnt}\",\n",
    "                    \"index_ids\": list(set(combined[\"index_ids\"])),\n",
    "                    \"article_ids_unique\": list(set(combined[\"article_ids\"])),\n",
    "                    \"titles_unique\": list(set(combined[\"titles\"])),\n",
    "                    \"content_contributors\": list(set(combined[\"pr_names\"])),\n",
    "                    \"urls_unique\": list(set(combined[\"urls\"])),\n",
    "                    \"chunks\": content,\n",
    "                    \"question\": question,\n",
    "                    \"answer\": response_text_ans,\n",
    "                }\n",
    "            )\n",
    "        df = pd.concat([df, pd.DataFrame(data)], ignore_index=True)\n",
    "        cnt += 1\n",
    "    return df, input_tokens, output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(df):\n",
    "    # get date, time of export\n",
    "    now = datetime.now()\n",
    "    today_date = now.strftime(\"%Y-%m-%d\")\n",
    "    current_time = now.strftime(\"%H-%M\") \n",
    "\n",
    "    output_dir = 'output'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    df.to_parquet(f\"output/question_bank_{today_date}_{current_time}.parquet\",index=False)\n",
    "    df.to_excel(f\"output/question_bank_{today_date}_{current_time}.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "qns_generation_prompt = \"\"\"Your task is to formulate a set of 3 unique questions from given context, satisfying the rules given below:\n",
    "1. All generated questions should precisely pertain to the keywords {keyword}, and it is imperative that the topic is explicitly included as an integral part of each question.\n",
    "2. The generated questions should be straightforward, using simple language that is accessible to a broad audience. \n",
    "3. The generated questions should make sense to humans even when read without the given context.\n",
    "4. Prioritize clarity and brevity, ensuring that the questions are formulated in a way that reflect common language and would be easily comprehensible to the general public. \n",
    "5. Ensure that the questions generated are meaningful and relevant to the general public in understanding or exploring more about the given topic.\n",
    "7. Only generate questions that can be derived from the given context, including text and tables.\n",
    "8. Importantly, ensure uniqueness and non-repetition in the questions. \n",
    "9. Additionally, all questions must have answers found within the given context.\n",
    "10. Do not use phrases like 'provided context', etc in the generated questions.\n",
    "11. A generated question should contain less than 15 words.\n",
    "12. Use simple language in the questions generated that are accessible to a broad audience.\n",
    "13. The question should be in first-person perspective.\n",
    "13. Each question should be enclosed in ' '.\n",
    "14. Output as a list of questions separated by , and enclosed by [ ]. \n",
    "\n",
    "Example of output:\n",
    "['How can MediSave be used for outpatient treatments for newborns?', 'What are the MediSave withdrawal limits for assisted conception procedures?', 'How does MediShield Life help with payments for costly outpatient treatments?']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kws = pd.read_excel(\"Automation Test Set of Keywords for Qns Generation.xlsx\")\n",
    "df_kws.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "for i in tqdm(range(0, len(df_kws))):\n",
    "    results_final=create_df()\n",
    "    for index,row in df_kws.iterrows():\n",
    "        method = row['method']\n",
    "        content_category = row['content_category']\n",
    "        subpage = row['subpage']\n",
    "        keywords_list = row['final_keywords']\n",
    "\n",
    "        if method == \"by_content_category\":\n",
    "            if content_category.strip() == '[programs, program-sub-pages]':\n",
    "                if subpage == \"vaccinate\":\n",
    "                    filter = build_filter_article_search(1434610) #article_id of Vaccinate programs page with js\n",
    "                else:\n",
    "                    filter = \"content_category eq 'programs' or content_category eq 'program-sub-pages'\"\n",
    "            else:\n",
    "                filter = build_filter_by_content_category(content_category)\n",
    "            \n",
    "            results_kws, input_tokens_log, output_tokens_log = asyncio.run(generate_by_content_category(keywords_list))\n",
    "\n",
    "        elif method == \"by_pg_views\":\n",
    "            if content_category == \"health-statistics\":\n",
    "                percentile = 0.75   # 4 out of 15 articles\n",
    "            elif content_category == \"medications\":\n",
    "                percentile = 0.95   # 29 out of 579 articles\n",
    "\n",
    "            df_percentile = get_articles_df(content_category, percentile)\n",
    "\n",
    "            results_kws, input_tokens_log, output_tokens_log = asyncio.run(generate_by_page_views(df_percentile, keywords_list))\n",
    "            \n",
    "        results_final = pd.concat([results_final, results_kws], ignore_index=True)\n",
    "        total_input_tokens += input_tokens_log\n",
    "        total_output_tokens += output_tokens_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rates from https://openai.com/api/pricing/\n",
    "cost_per_million_input_tokens = 5\n",
    "cost_per_million_output_tokens = 15\n",
    "cost_input = (total_input_tokens / 1000000) * cost_per_million_input_tokens\n",
    "cost_output = (total_output_tokens / 1000000) * cost_per_million_output_tokens\n",
    "\n",
    "print(\"Number of tokens\")\n",
    "print(f\"total_input_tokens: {total_input_tokens}\")\n",
    "print(f\"total_output_tokens: {total_output_tokens}\")\n",
    "print(f\"\\nTotal cost\")\n",
    "print(f\"input: ${round(cost_input,4)}\")\n",
    "print(f\"output: ${round(cost_output,4)}\")\n",
    "print(f\"total: ${round(cost_input+cost_output,4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of duplicate questions: 0\n",
      "No. of duplicate rows: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Joycelyn\\AppData\\Local\\Temp\\ipykernel_33364\\2187584482.py:1: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_copy = results_final.applymap(lambda x: str(x) if isinstance(x, list) else x)\n"
     ]
    }
   ],
   "source": [
    "df_copy = results_final.applymap(lambda x: str(x) if isinstance(x, list) else x)\n",
    "duplicate_qns = df_copy[df_copy.duplicated(subset=\"question\", keep=False)]\n",
    "duplicate_row = df_copy[df_copy.duplicated(keep=False)]\n",
    "print(f\"No. of duplicate questions: {len(duplicate_qns)}\")\n",
    "print(f\"No. of duplicate rows: {len(duplicate_row)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_final.shape)\n",
    "results_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_final = results_final.drop_duplicates(subset='question')\n",
    "# results_final.shape\n",
    "# save_file(results_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-hh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
