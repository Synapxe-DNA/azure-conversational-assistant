{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(dotenv_path=r'..\\.azure\\hhgai-dev-eastus-001\\.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.identity.aio import DefaultAzureCredential, get_bearer_token_provider\n",
    "from azure.search.documents.aio import SearchClient\n",
    "\n",
    "from typing import TypedDict, Optional\n",
    "from openai import AsyncAzureOpenAI\n",
    "\n",
    "from openai_messages_token_helper import build_messages, get_token_limit\n",
    "from openai.types.chat import (\n",
    "    ChatCompletion,\n",
    "    ChatCompletionMessageParam,\n",
    "    ChatCompletionToolParam,\n",
    ")\n",
    "\n",
    "from azure.search.documents.models import (\n",
    "    QueryCaptionResult,\n",
    "    QueryType,\n",
    "    VectorizedQuery,\n",
    "    VectorQuery,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_SEARCH_SERVICE = os.environ[\"AZURE_SEARCH_SERVICE\"]\n",
    "AZURE_SEARCH_INDEX = os.environ[\"AZURE_SEARCH_INDEX\"]\n",
    "OPENAI_HOST = os.getenv(\"OPENAI_HOST\", \"azure\")\n",
    "OPENAI_CHATGPT_MODEL = os.environ[\"AZURE_OPENAI_CHATGPT_MODEL\"]\n",
    "OPENAI_EMB_MODEL = os.getenv(\"AZURE_OPENAI_EMB_MODEL_NAME\", \"text-embedding-ada-002\")\n",
    "OPENAI_EMB_DIMENSIONS = int(os.getenv(\"AZURE_OPENAI_EMB_DIMENSIONS\", 1536))\n",
    "AZURE_OPENAI_SERVICE = os.getenv(\"AZURE_OPENAI_SERVICE\")\n",
    "AZURE_OPENAI_CHATGPT_DEPLOYMENT = (\n",
    "    os.getenv(\"AZURE_OPENAI_CHATGPT_DEPLOYMENT\") if OPENAI_HOST.startswith(\"azure\") else None\n",
    ")\n",
    "AZURE_OPENAI_API_VERSION = os.getenv(\"AZURE_OPENAI_API_VERSION\") or \"2024-03-01-preview\"\n",
    "AZURE_OPENAI_EMB_DEPLOYMENT = os.getenv(\"AZURE_OPENAI_EMB_DEPLOYMENT\") if OPENAI_HOST.startswith(\"azure\") else None\n",
    "AZURE_OPENAI_CUSTOM_URL = os.getenv(\"AZURE_OPENAI_CUSTOM_URL\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_ORGANIZATION = os.getenv(\"OPENAI_ORGANIZATION\")\n",
    "AZURE_TENANT_ID = os.getenv(\"AZURE_TENANT_ID\")\n",
    "AZURE_USE_AUTHENTICATION = os.getenv(\"AZURE_USE_AUTHENTICATION\", \"\").lower() == \"true\"\n",
    "AZURE_ENFORCE_ACCESS_CONTROL = os.getenv(\"AZURE_ENFORCE_ACCESS_CONTROL\", \"\").lower() == \"true\"\n",
    "AZURE_ENABLE_GLOBAL_DOCUMENT_ACCESS = os.getenv(\"AZURE_ENABLE_GLOBAL_DOCUMENT_ACCESS\", \"\").lower() == \"true\"\n",
    "AZURE_ENABLE_UNAUTHENTICATED_ACCESS = os.getenv(\"AZURE_ENABLE_UNAUTHENTICATED_ACCESS\", \"\").lower() == \"true\"\n",
    "AZURE_SERVER_APP_ID = os.getenv(\"AZURE_SERVER_APP_ID\")\n",
    "AZURE_SERVER_APP_SECRET = os.getenv(\"AZURE_SERVER_APP_SECRET\")\n",
    "AZURE_CLIENT_APP_ID = os.getenv(\"AZURE_CLIENT_APP_ID\")\n",
    "AZURE_AUTH_TENANT_ID = os.getenv(\"AZURE_AUTH_TENANT_ID\", AZURE_TENANT_ID)\n",
    "\n",
    "AZURE_SEARCH_QUERY_LANGUAGE = os.getenv(\"AZURE_SEARCH_QUERY_LANGUAGE\", \"en-us\")\n",
    "AZURE_SEARCH_QUERY_SPELLER = os.getenv(\"AZURE_SEARCH_QUERY_SPELLER\", \"lexicon\")\n",
    "AZURE_SEARCH_SEMANTIC_RANKER = os.getenv(\"AZURE_SEARCH_SEMANTIC_RANKER\", \"free\").lower()\n",
    "\n",
    "CHATGPT_TOKEN_LIMIT = get_token_limit(OPENAI_CHATGPT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_credential = DefaultAzureCredential(exclude_shared_token_cache_credential=True)\n",
    "token_provider = get_bearer_token_provider(azure_credential, \"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "openai_client = AsyncAzureOpenAI(\n",
    "    api_version=AZURE_OPENAI_API_VERSION,\n",
    "    azure_endpoint=f\"https://{AZURE_OPENAI_SERVICE}.openai.azure.com\",\n",
    "    azure_ad_token_provider=token_provider,\n",
    ")\n",
    "\n",
    "search_client = SearchClient(\n",
    "    endpoint=f\"https://{AZURE_SEARCH_SERVICE}.search.windows.net\",\n",
    "    index_name=AZURE_SEARCH_INDEX,\n",
    "    credential=azure_credential,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "SEARCH_MAX_RESULTS = 10\n",
    "TEMPERATURE = 0.0\n",
    "SEED = 1234\n",
    "USE_TEXT_SEARCH = False\n",
    "USE_VECTOR_SEARCH = True\n",
    "USE_SEMANTIC_RANKER = False\n",
    "USE_SEMANTIC_CAPTIONS = False\n",
    "MINIMUM_SEARCH_SCORE = 0.0\n",
    "MINIMUM_RERANKER_SCORE = 0.0\n",
    "\n",
    "RESPONSE_TOKEN_LIMIT = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    id: Optional[str]\n",
    "    parent_id: Optional[str]\n",
    "    title: Optional[str]\n",
    "    cover_image_url: Optional[str]\n",
    "    full_url: Optional[str]\n",
    "    content_category: Optional[str]\n",
    "    chunks: Optional[str]\n",
    "    embedding: Optional[list[float]]\n",
    "    captions: list[QueryCaptionResult]\n",
    "    score: Optional[float] = None\n",
    "    reranker_score: Optional[float] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def compute_text_embedding(q: str):\n",
    "    SUPPORTED_DIMENSIONS_MODEL = {\n",
    "        \"text-embedding-ada-002\": False,\n",
    "        \"text-embedding-3-small\": True,\n",
    "        \"text-embedding-3-large\": True,\n",
    "    }\n",
    "\n",
    "    class ExtraArgs(TypedDict, total=False):\n",
    "        dimensions: int\n",
    "\n",
    "    dimensions_args: ExtraArgs = (\n",
    "        {\"dimensions\": OPENAI_EMB_DIMENSIONS} if SUPPORTED_DIMENSIONS_MODEL[OPENAI_EMB_MODEL] else {}\n",
    "    )\n",
    "    embedding = await openai_client.embeddings.create(\n",
    "        # Azure OpenAI takes the deployment name as the model name\n",
    "        model=AZURE_OPENAI_EMB_DEPLOYMENT if AZURE_OPENAI_EMB_DEPLOYMENT else OPENAI_EMB_MODEL,\n",
    "        input=q,\n",
    "        **dimensions_args,\n",
    "    )\n",
    "    query_vector = embedding.data[0].embedding\n",
    "    return VectorizedQuery(vector=query_vector, k_nearest_neighbors=50, fields=\"embedding\")\n",
    "\n",
    "def get_citation(sourcepage: str, use_image_citation: bool) -> str:\n",
    "    if use_image_citation:\n",
    "        return sourcepage\n",
    "    else:\n",
    "        path, ext = os.path.splitext(sourcepage)\n",
    "        if ext.lower() == \".png\":\n",
    "            page_idx = path.rfind(\"-\")\n",
    "            page_number = int(path[page_idx + 1 :])\n",
    "            return f\"{path[:page_idx]}.pdf#page={page_number}\"\n",
    "\n",
    "        return sourcepage\n",
    "\n",
    "def nonewlines(s: str) -> str:\n",
    "    return s.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    \n",
    "async def get_sources_content(results: list[Document], use_semantic_captions: bool, use_image_citation: bool) -> list[str]:\n",
    "    if use_semantic_captions:\n",
    "        return [\n",
    "            (get_citation((doc['full_url'] or \"\"), use_image_citation))\n",
    "            + \": \"\n",
    "            + nonewlines(\" . \".join([cast(str, c.text) for c in (doc['captions'] or [])]))\n",
    "            async for doc in results\n",
    "        ]\n",
    "    else:\n",
    "        return [\n",
    "            (get_citation((doc['full_url'] or \"\"), use_image_citation)) + \": \" + nonewlines(doc['chunks'] or \"\")\n",
    "            async for doc in results\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "qns_generation_prompt = \"\"\"Your task is to formulate a set of 2 unique questions from given context, satisfying the rules given below:\n",
    "1. All generated questions should precisely pertain to the topic of {subtopic}, and it is imperative that the topic is explicitly included as an integral part of each question.\n",
    "2. The generated questions should be straightforward, using simple language that is accessible to a broad audience. \n",
    "3. The generated questions should make sense to humans even when read without the given context.\n",
    "4. Prioritize clarity and brevity, ensuring that the questions are formulated in a way that reflect common language and would be easily comprehensible to the general public. \n",
    "5. Ensure that the questions generated are meaningful and relevant to the general public in understanding or exploring more about the given topic.\n",
    "7. Only generate questions that can be derived from the given context, including text and tables.\n",
    "8. Importantly, ensure uniqueness and non-repetition in the questions. \n",
    "9. Additionally, all questions must have answers found within the given context.\n",
    "10. Do not use phrases like 'provided context', etc in the generated questions.\n",
    "11. A generated question should contain less than 15 words.\n",
    "12. Each question must be followed with the source url.\n",
    "13. Use simple language in the questions generated that are accessible to a broad audience.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\"Diabetes\",\"Breast cancer\",\"Vaccination\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diabetes\n",
      "- number of sources retrieved: 10\n",
      "- number of qns generated for source 1: 2\n",
      "- number of qns generated for source 2: 2\n",
      "- number of qns generated for source 3: 2\n",
      "- number of qns generated for source 4: 2\n",
      "- number of qns generated for source 5: 2\n",
      "- number of qns generated for source 6: 2\n",
      "- number of qns generated for source 7: 2\n",
      "- number of qns generated for source 8: 1\n",
      "- number of qns generated for source 9: 2\n",
      "- number of qns generated for source 10: 2\n",
      "Breast cancer\n",
      "- number of sources retrieved: 10\n",
      "- number of qns generated for source 1: 2\n",
      "- number of qns generated for source 2: 2\n",
      "- number of qns generated for source 3: 2\n",
      "- number of qns generated for source 4: 2\n",
      "- number of qns generated for source 5: 2\n",
      "- number of qns generated for source 6: 2\n",
      "- number of qns generated for source 7: 2\n",
      "- number of qns generated for source 8: 2\n",
      "- number of qns generated for source 9: 2\n",
      "- number of qns generated for source 10: 2\n",
      "Vaccination\n",
      "- number of sources retrieved: 10\n",
      "- number of qns generated for source 1: 2\n",
      "- number of qns generated for source 2: 2\n",
      "- number of qns generated for source 3: 2\n",
      "- number of qns generated for source 4: 2\n",
      "- number of qns generated for source 5: 2\n",
      "- number of qns generated for source 6: 2\n",
      "- number of qns generated for source 7: 2\n",
      "- number of qns generated for source 8: 2\n",
      "- number of qns generated for source 9: 2\n",
      "- number of qns generated for source 10: 2\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns = [\"Topic\", \"Question\", \"URL\", \"Chunk\"])\n",
    "\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    vectors: list[VectorQuery] = []\n",
    "    if USE_VECTOR_SEARCH:\n",
    "        vectors.append(await compute_text_embedding(topic))\n",
    "\n",
    "    if USE_SEMANTIC_RANKER:\n",
    "        results = await search_client.search(\n",
    "            search_text=topic,\n",
    "            filter=None,\n",
    "            top=SEARCH_MAX_RESULTS,\n",
    "            query_caption=\"extractive|highlight-false\" if USE_SEMANTIC_CAPTIONS else None,\n",
    "            vector_queries=vectors,\n",
    "            query_type=QueryType.SEMANTIC,\n",
    "            query_language=AZURE_SEARCH_QUERY_LANGUAGE,\n",
    "            query_speller=AZURE_SEARCH_QUERY_SPELLER,\n",
    "            semantic_configuration_name=\"default\",\n",
    "            semantic_query=topic,\n",
    "        )\n",
    "    else:\n",
    "        results = await search_client.search(\n",
    "            search_text=topic,\n",
    "            filter=None,\n",
    "            top=SEARCH_MAX_RESULTS,\n",
    "            vector_queries=vectors,\n",
    "        )\n",
    "    \n",
    "    sources_content = await get_sources_content(results, USE_SEMANTIC_CAPTIONS, use_image_citation=False)\n",
    "    print(f\"- number of sources retrieved: {len(sources_content)}\")\n",
    "\n",
    "    cnt=1\n",
    "    for source in sources_content:\n",
    "        messages = build_messages(\n",
    "            model=OPENAI_CHATGPT_MODEL,\n",
    "            system_prompt=qns_generation_prompt.format(subtopic=topic),\n",
    "            new_user_content=f\"Please generate 2 unique questions on topic '{topic}' using the following provided source. \\n\\nSource:\\n {source}\",\n",
    "            max_tokens=CHATGPT_TOKEN_LIMIT - RESPONSE_TOKEN_LIMIT,\n",
    "        )\n",
    "        chat_coroutine = await openai_client.chat.completions.create(\n",
    "            # Azure OpenAI takes the deployment name as the model name\n",
    "            model=AZURE_OPENAI_CHATGPT_DEPLOYMENT,\n",
    "            messages=messages,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=RESPONSE_TOKEN_LIMIT,\n",
    "            n=1,\n",
    "            stream=False,\n",
    "            seed=SEED,\n",
    "        )\n",
    "        \n",
    "        response_text = chat_coroutine.choices[0].message.content\n",
    "        entries = response_text.strip().split(\"\\n\\n\")\n",
    "        print(f\"- number of qns generated for source {cnt}: {len(entries)}\")\n",
    "        \n",
    "        data = []\n",
    "        for entry in entries:\n",
    "            entry = entry.replace(\"Source:\",\"\")\n",
    "            question, url = entry.split('https://', 1)\n",
    "            question = question.split(\". \",1)[1].strip()\n",
    "            data.append({\"Topic\": topic, \"Question\": question, \"URL\": url, \"Chunk\": source})\n",
    "        df = pd.concat([df, pd.DataFrame(data)], ignore_index=True)\n",
    "        cnt+=1\n",
    "\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df.drop_duplicates(subset=['Question'])\n",
    "df_unique.to_csv(\"generated_questions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-hh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
